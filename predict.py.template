# Standard
from typing import NamedTuple, Optional
import asyncio
import base64
import os
import subprocess
import time

# Third Party
from openai import AsyncOpenAI
import httpx

# First Party
from cog import BasePredictor, ConcatenateIterator, Input, Path

# Local
from utils import resolve_model_path


TRITON_START_TIMEOUT_MINUTES = 5
VLLM_ENV = os.environ.get("PYTHON_VLLM", "/vllm-env")

## Template ####################################################################
# This is the key template where the static model weight URLs will be placed.
# The wrapper script will replace this with either a single string or a list of
# strings that will be resolved with resolve_model_path.
MODEL_WEIGHTS = {{ MODEL_WEIGHTS_VAR }}
################################################################################

class PredictorConfig(NamedTuple):
    prompt_template: Optional[str] = None


class Predictor(BasePredictor):
    async def start_vllm(self, model: str, args: dict) -> bool:
        client = httpx.AsyncClient()
        cmd = [f"{VLLM_ENV}/bin/vllm", "serve", model, "--disable-log-requests"]
        for k, v in args.items():
            cmd.extend(("--" + k, str(v)))
        self.proc = subprocess.Popen(cmd)
        # Health check Triton until it is ready or for 5 minutes
        for i in range(TRITON_START_TIMEOUT_MINUTES * 60):
            try:
                response = await client.get(
                    "http://localhost:8000/health"
                )
                if response.status_code == 200:
                    print("VLLM is ready.")
                    return True
            except httpx.RequestError:
                pass
            await asyncio.sleep(1)
        print(f"VLLM was not ready within {TRITON_START_TIMEOUT_MINUTES} minutes (exit code: {self.proc.poll()})")
        self.proc.terminate()
        await asyncio.sleep(0.001)
        self.proc.kill()
        return False

    async def setup(self):
        weights = await resolve_model_path(MODEL_WEIGHTS)
        res = await self.start_vllm(
            weights,
            {
                "dtype": "auto",
                "tensor_parallel_size": 1,
            }
        )


        self.client = AsyncOpenAI(
            api_key="EMPTY",
            base_url="http://localhost:8000/v1"
        )
        models = await self.client.models.list()
        self.model_id = models.data[0].id

    async def predict(  # pylint: disable=invalid-overridden-method, arguments-differ
        self,
        prompt: str = Input(description="Prompt", default=""),
        image: Path = Input(description="Image input", default=None),
        system_prompt: str = Input(
            description="System prompt to send to the model. This is prepended to the prompt and helps guide system behavior. Ignored for non-chat models.",
            default="",
        ),
        min_tokens: int = Input(
            description="The minimum number of tokens the model should generate as output.",
            default=0,
        ),
        max_tokens: int = Input(
            description="The maximum number of tokens the model should generate as output.",
            default=512,
        ),
        temperature: float = Input(
            description="The value used to modulate the next token probabilities.",
            default=0.6,
        ),
        top_p: float = Input(
            description="A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).",
            default=0.9,
        ),
        top_k: int = Input(
            description="The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering).",
            default=50,
        ),
        presence_penalty: float = Input(description="Presence penalty", default=0.0),
        frequency_penalty: float = Input(description="Frequency penalty", default=0.0),
        stop_sequences: str = Input(
            description="A comma-separated list of sequences to stop generation at. For example, '<end>,<stop>' will stop generation at the first instance of 'end' or '<stop>'.",
            default=None,
        ),
    ) -> ConcatenateIterator[str]:
        start = time.time()

        system_prompt = "" if system_prompt is None else system_prompt
        if isinstance(stop_sequences, str) and stop_sequences:
            stop = stop_sequences.split(",")
        else:
            stop = (
                list(stop_sequences) if isinstance(stop_sequences, list) else []
            )

        # Construct the message sequence for the chat API
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        user_content = None
        if prompt and not image:
            user_content = prompt
        else:
            user_content = []
            if prompt:
                user_content.append({"type": "text", "text": prompt})
            if image:
                with open(image, "rb") as handle:
                    image_b64 = base64.b64encode(handle.read()).decode("utf-8")
                user_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image;base64,{image_b64}"}
                })
        if user_content:
            messages.append({"role": "user", "content": user_content})

        # TODO: figure out how to add top_k, min_tokens, frequency_penalty, presence_penalty, beam search
        chat_response = await self.client.chat.completions.create(
            model=self.model_id,
            # echo=False,
            n=1,
            stream=True,
            messages = messages,
            # top_k=(-1 if (top_k or 0) == 0 else top_k),
            top_p=top_p,
            stop=stop,
            temperature=temperature,
            # min_tokens=min_tokens,
            max_tokens=max_tokens,
            # frequency_penalty=frequency_penalty,
            # presence_penalty=presence_penalty,
            # use_beam_search=False,
        )
        async for completion in chat_response:
            yield completion.choices[0].delta.content

        self.log(f"Generation took {time.time() - start:.2f}s")
        self.log(f"Formatted prompt: {prompt}")
